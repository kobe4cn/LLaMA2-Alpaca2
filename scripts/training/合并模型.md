### 模型合并脚本调用方式
### 合并 chinese-llama-2-lora-7b与LLaMa2 hf
python scripts/merge_llama2_with_chinese_lora_low_mem.py \
--base_model /mnt/mydisk/llama2_models/Llama-2-7b-hf/ \
--lora_model /mnt/mydisk/llama2_models/chinese-llama-2-lora-7b/ \
--output_dir /mnt/mydisk/llama2-merage-output/merge-llama2-chinese-lora-7B/ \
--output_type huggingface

### LLaMa2开始不支持多个lora模型合并到原始的LLaMa2 hf模型上，只能LLama2原始模型与chinese-llama-2-lora-7b或者与chinese-alpaca-2-lora-7b 生成新的合并模型

### 合并 chinese-alpaca-2-lora-7b与LLaMa2 hf
python scripts/merge_llama2_with_chinese_lora_low_mem.py \
--base_model /mnt/mydisk/llama2_models/Llama-2-7b-hf/ \
--lora_model /mnt/mydisk/llama2_models/chinese-alpaca-2-lora-7b/ \
--output_dir /mnt/mydisk/llama2-merage-output/merge-llama2-chinese-alpaca-7B/ \
--output_type huggingface


手动模型合并与转换

以下介绍了手动将LoRA与原版Llama-2合并得到完整模型的流程。如网络带宽充足，建议直接下载完整版模型。

准备工作

运行前确保拉取仓库最新版代码：git pull
确保机器有足够的内存加载完整模型（例如7B模型需要13-15G）以进行合并模型操作
安装依赖库（项目根目录requirements.txt）：
$ pip install -r requirements.txt
Step 1: 获取原版Llama-2-hf模型

原版Llama-2-hf地址：https://huggingface.co/meta-llama/Llama-2-7b-hf

HF格式模型相关文件（可以不用下载safetensors格式模型权重）：

config.json
generation_config.json
pytorch_model-00001-of-00002.bin
pytorch_model-00002-of-00002.bin
pytorch_model.bin.index.json
special_tokens_map.json
tokenizer_config.json
tokenizer.json
tokenizer.model
Step 2: 合并LoRA权重，生成全量模型权重

这一步骤会合并LoRA权重，生成全量模型权重。此处可以选择输出PyTorch版本权重（.pth文件）或者输出HuggingFace版本权重（.bin文件）。执行以下命令：

$ python scripts/merge_llama2_with_chinese_lora_low_mem.py \
    --base_model path_to_original_llama2_hf_dir \
    --lora_model path_to_chinese_llama2_or_alpaca2_lora \
    --output_type huggingface \
    --output_dir path_to_output_dir 
参数说明：

--base_model：存放HF格式的Llama-2模型权重和配置文件的目录
--lora_model：中文LLaMA-2/Alpaca-2 LoRA解压后文件所在目录，也可使用🤗Model Hub模型调用名称（会自动下载）
--output_type：指定输出格式，可为pth或huggingface。若不指定，默认为huggingface
--output_dir：指定保存全量模型权重的目录，默认为./
（可选）--verbose：显示合并过程中的详细信息